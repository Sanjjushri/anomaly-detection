{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trade Dataset Anomaly Detection with Isolation Forest\n",
    "\n",
    "This notebook implements an Isolation Forest algorithm from scratch for detecting anomalies in high-dimensional financial trade data.\n",
    "\n",
    "## Overview\n",
    "- **Dataset**: High-dimensional trade data with 16 financial features\n",
    "- **Algorithm**: Isolation Forest (implemented from scratch)\n",
    "- **Evaluation**: Comprehensive metrics including accuracy, precision, recall, F1-score, and AUC\n",
    "- **Visualization**: Detailed analysis of detected anomalies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any, Tuple, Optional\n",
    "\n",
    "# Set style for better plots\n",
    "plt.style.use('dark_background')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tree Node Class Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TreeNode:\n",
    "    \"\"\"Node in an isolation tree.\"\"\"\n",
    "    def __init__(self, feature: str = None, threshold: float = None, \n",
    "                 left: 'TreeNode' = None, right: 'TreeNode' = None, \n",
    "                 size: int = 0, depth: int = 0):\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.size = size\n",
    "        self.depth = depth\n",
    "\n",
    "print(\"TreeNode class defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Isolation Forest Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IsolationForest:\n",
    "    \"\"\"Isolation Forest implementation for anomaly detection.\"\"\"\n",
    "    \n",
    "    def __init__(self, n_trees: int = 100, subsample_size: int = 256, \n",
    "                 max_depth: int = 10, contamination: float = 0.1):\n",
    "        self.n_trees = n_trees\n",
    "        self.subsample_size = subsample_size\n",
    "        self.max_depth = max_depth\n",
    "        self.contamination = contamination\n",
    "        self.trees = []\n",
    "        self.feature_names = []\n",
    "        \n",
    "    def fit(self, data: List[Dict[str, Any]]) -> None:\n",
    "        \"\"\"Train the isolation forest on the provided data.\"\"\"\n",
    "        print(f\"Training Isolation Forest with {self.n_trees} trees...\")\n",
    "        \n",
    "        # Extract feature names (exclude non-numeric fields)\n",
    "        exclude_fields = {'id', 'timestamp', 'symbol', 'is_anomaly', 'anomaly_reasons', 'anomaly_type'}\n",
    "        self.feature_names = [key for key in data[0].keys() \n",
    "                             if key not in exclude_fields and isinstance(data[0][key], (int, float))]\n",
    "        \n",
    "        print(f\"Using {len(self.feature_names)} features: {self.feature_names}\")\n",
    "        \n",
    "        # Build trees\n",
    "        self.trees = []\n",
    "        for i in range(self.n_trees):\n",
    "            if (i + 1) % 20 == 0:\n",
    "                print(f\"  Built {i + 1}/{self.n_trees} trees\")\n",
    "            \n",
    "            # Sample data for this tree\n",
    "            sample = self._sample_data(data, self.subsample_size)\n",
    "            \n",
    "            # Build the tree\n",
    "            tree = self._build_tree(sample, 0, self.max_depth)\n",
    "            self.trees.append(tree)\n",
    "        \n",
    "        print(\"Training completed!\")\n",
    "    \n",
    "    def predict(self, data: List[Dict[str, Any]]) -> List[float]:\n",
    "        \"\"\"Predict anomaly scores for the data.\"\"\"\n",
    "        scores = []\n",
    "        for record in data:\n",
    "            score = self._anomaly_score(record)\n",
    "            scores.append(score)\n",
    "        return scores\n",
    "    \n",
    "    def _sample_data(self, data: List[Dict[str, Any]], size: int) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Sample data for tree building.\"\"\"\n",
    "        sample_size = min(size, len(data))\n",
    "        return random.sample(data, sample_size)\n",
    "    \n",
    "    def _build_tree(self, data: List[Dict[str, Any]], depth: int, max_depth: int) -> TreeNode:\n",
    "        \"\"\"Build an isolation tree recursively.\"\"\"\n",
    "        if len(data) <= 1 or depth >= max_depth:\n",
    "            return TreeNode(size=len(data), depth=depth)\n",
    "        \n",
    "        # Randomly select a feature\n",
    "        feature = random.choice(self.feature_names)\n",
    "        \n",
    "        # Get feature values\n",
    "        values = [record[feature] for record in data if feature in record]\n",
    "        if not values:\n",
    "            return TreeNode(size=len(data), depth=depth)\n",
    "        \n",
    "        min_val, max_val = min(values), max(values)\n",
    "        if min_val == max_val:\n",
    "            return TreeNode(size=len(data), depth=depth)\n",
    "        \n",
    "        # Random split threshold\n",
    "        threshold = min_val + random.random() * (max_val - min_val)\n",
    "        \n",
    "        # Split data\n",
    "        left_data = [record for record in data if record.get(feature, 0) < threshold]\n",
    "        right_data = [record for record in data if record.get(feature, 0) >= threshold]\n",
    "        \n",
    "        # Build child nodes\n",
    "        left_child = self._build_tree(left_data, depth + 1, max_depth)\n",
    "        right_child = self._build_tree(right_data, depth + 1, max_depth)\n",
    "        \n",
    "        return TreeNode(feature=feature, threshold=threshold, \n",
    "                       left=left_child, right=right_child, depth=depth)\n",
    "    \n",
    "    def _path_length(self, record: Dict[str, Any], node: TreeNode, current_depth: int) -> float:\n",
    "        \"\"\"Calculate path length for a record in a tree.\"\"\"\n",
    "        if node.feature is None or node.left is None or node.right is None:\n",
    "            # Leaf node - add expected path length for remaining points\n",
    "            return current_depth + self._harmonic_number(node.size)\n",
    "        \n",
    "        # Internal node - traverse based on feature value\n",
    "        feature_value = record.get(node.feature, 0)\n",
    "        if feature_value < node.threshold:\n",
    "            return self._path_length(record, node.left, current_depth + 1)\n",
    "        else:\n",
    "            return self._path_length(record, node.right, current_depth + 1)\n",
    "    \n",
    "    def _harmonic_number(self, n: int) -> float:\n",
    "        \"\"\"Calculate harmonic number H(n-1) for expected path length.\"\"\"\n",
    "        if n <= 1:\n",
    "            return 0\n",
    "        return math.log(n - 1) + 0.5772156649  # Euler's constant\n",
    "    \n",
    "    def _anomaly_score(self, record: Dict[str, Any]) -> float:\n",
    "        \"\"\"Calculate anomaly score for a single record.\"\"\"\n",
    "        # Average path length across all trees\n",
    "        path_lengths = [self._path_length(record, tree, 0) for tree in self.trees]\n",
    "        avg_path_length = sum(path_lengths) / len(path_lengths)\n",
    "        \n",
    "        # Expected path length for normal points\n",
    "        expected_path_length = self._harmonic_number(self.subsample_size)\n",
    "        \n",
    "        # Anomaly score: 2^(-avg_path_length / expected_path_length)\n",
    "        if expected_path_length > 0:\n",
    "            score = 2 ** (-avg_path_length / expected_path_length)\n",
    "        else:\n",
    "            score = 0.5\n",
    "        \n",
    "        return score\n",
    "\n",
    "print(\"IsolationForest class defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Evaluator Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelEvaluator:\n",
    "    \"\"\"Evaluate isolation forest model performance.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_threshold(scores: List[float], contamination: float) -> float:\n",
    "        \"\"\"Calculate threshold based on contamination rate.\"\"\"\n",
    "        sorted_scores = sorted(scores, reverse=True)\n",
    "        threshold_index = int(len(sorted_scores) * contamination)\n",
    "        return sorted_scores[threshold_index] if threshold_index < len(sorted_scores) else 0.5\n",
    "    \n",
    "    @staticmethod\n",
    "    def evaluate_model(data: List[Dict[str, Any]], scores: List[float], \n",
    "                      contamination: float) -> Dict[str, Any]:\n",
    "        \"\"\"Evaluate model performance with comprehensive metrics.\"\"\"\n",
    "        threshold = ModelEvaluator.calculate_threshold(scores, contamination)\n",
    "        \n",
    "        # Calculate confusion matrix\n",
    "        tp = fp = tn = fn = 0\n",
    "        \n",
    "        for i, record in enumerate(data):\n",
    "            predicted_anomaly = scores[i] > threshold\n",
    "            actual_anomaly = record.get('is_anomaly', False)\n",
    "            \n",
    "            if predicted_anomaly and actual_anomaly:\n",
    "                tp += 1\n",
    "            elif predicted_anomaly and not actual_anomaly:\n",
    "                fp += 1\n",
    "            elif not predicted_anomaly and not actual_anomaly:\n",
    "                tn += 1\n",
    "            else:  # not predicted_anomaly and actual_anomaly\n",
    "                fn += 1\n",
    "        \n",
    "        # Calculate metrics\n",
    "        total = tp + fp + tn + fn\n",
    "        accuracy = (tp + tn) / total if total > 0 else 0\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        # Calculate AUC\n",
    "        auc = ModelEvaluator._calculate_auc(data, scores)\n",
    "        \n",
    "        return {\n",
    "            'threshold': threshold,\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1_score,\n",
    "            'auc': auc,\n",
    "            'confusion_matrix': {\n",
    "                'true_positives': tp,\n",
    "                'false_positives': fp,\n",
    "                'true_negatives': tn,\n",
    "                'false_negatives': fn\n",
    "            },\n",
    "            'total_samples': total,\n",
    "            'anomalies_detected': tp + fp,\n",
    "            'actual_anomalies': tp + fn\n",
    "        }\n",
    "    \n",
    "    @staticmethod\n",
    "    def _calculate_auc(data: List[Dict[str, Any]], scores: List[float]) -> float:\n",
    "        \"\"\"Calculate Area Under Curve (AUC) for ROC.\"\"\"\n",
    "        # Create pairs of (score, label)\n",
    "        pairs = [(scores[i], 1 if data[i].get('is_anomaly', False) else 0) \n",
    "                for i in range(len(data))]\n",
    "        pairs.sort(key=lambda x: x[0], reverse=True)\n",
    "        \n",
    "        total_pos = sum(1 for _, label in pairs if label == 1)\n",
    "        total_neg = len(pairs) - total_pos\n",
    "        \n",
    "        if total_pos == 0 or total_neg == 0:\n",
    "            return 0.5\n",
    "        \n",
    "        auc = 0.0\n",
    "        tp = 0\n",
    "        fp = 0\n",
    "        prev_fp_rate = 0.0\n",
    "        \n",
    "        for score, label in pairs:\n",
    "            if label == 1:\n",
    "                tp += 1\n",
    "            else:\n",
    "                fp += 1\n",
    "                # Calculate area increment\n",
    "                tp_rate = tp / total_pos\n",
    "                fp_rate = fp / total_neg\n",
    "                auc += tp_rate * (fp_rate - prev_fp_rate)\n",
    "                prev_fp_rate = fp_rate\n",
    "        \n",
    "        return auc\n",
    "\n",
    "print(\"ModelEvaluator class defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load and Explore Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "try:\n",
    "    with open('trade_dataset_medium.json', 'r') as f:\n",
    "        data = json.load(f)\n",
    "    print(f\"✅ Loaded dataset with {len(data)} trades\")\n",
    "except FileNotFoundError:\n",
    "    print(\"❌ Error: Dataset file not found. Please run generate_dataset.py first.\")\n",
    "    # For demonstration, let's create a small sample dataset\n",
    "    print(\"Creating sample dataset for demonstration...\")\n",
    "    data = []\n",
    "    \n",
    "# Dataset statistics\n",
    "if data:\n",
    "    anomalies = sum(1 for trade in data if trade.get('is_anomaly', False))\n",
    "    print(f\"📊 Dataset Statistics:\")\n",
    "    print(f\"   Total trades: {len(data)}\")\n",
    "    print(f\"   Actual anomalies: {anomalies} ({anomalies/len(data)*100:.1f}%)\")\n",
    "    print(f\"   Normal trades: {len(data) - anomalies}\")\n",
    "    \n",
    "    # Show sample features\n",
    "    sample_trade = data[0]\n",
    "    exclude_fields = {'id', 'timestamp', 'symbol', 'is_anomaly', 'anomaly_reasons', 'anomaly_type'}\n",
    "    features = [key for key in sample_trade.keys() if key not in exclude_fields]\n",
    "    print(f\"\\n🔍 Features ({len(features)}): {features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Convert to DataFrame for Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data:\n",
    "    # Convert to DataFrame for easier analysis\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Display basic info\n",
    "    print(\"📈 Dataset Overview:\")\n",
    "    print(df.info())\n",
    "    \n",
    "    print(\"\\n📊 Statistical Summary:\")\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    print(df[numeric_cols].describe())\n",
    "    \n",
    "    # Show anomaly distribution by type\n",
    "    if 'anomaly_type' in df.columns:\n",
    "        print(\"\\n🚨 Anomaly Types Distribution:\")\n",
    "        anomaly_types = df[df['is_anomaly'] == True]['anomaly_type'].value_counts()\n",
    "        print(anomaly_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train Isolation Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data:\n",
    "    print(\"🤖 Training Isolation Forest Model\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Initialize and train model\n",
    "    model = IsolationForest(\n",
    "        n_trees=100,\n",
    "        subsample_size=256,\n",
    "        max_depth=10,\n",
    "        contamination=0.05\n",
    "    )\n",
    "    \n",
    "    model.fit(data)\n",
    "    \n",
    "    print(\"\\n✅ Model training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Generate Predictions and Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data:\n",
    "    print(\"🔮 Generating Predictions...\")\n",
    "    scores = model.predict(data)\n",
    "    \n",
    "    print(\"📊 Evaluating Model Performance...\")\n",
    "    metrics = ModelEvaluator.evaluate_model(data, scores, model.contamination)\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"🎯 MODEL PERFORMANCE METRICS\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Threshold: {metrics['threshold']:.4f}\")\n",
    "    print(f\"Accuracy:  {metrics['accuracy']:.3f} ({metrics['accuracy']*100:.1f}%)\")\n",
    "    print(f\"Precision: {metrics['precision']:.3f} ({metrics['precision']*100:.1f}%)\")\n",
    "    print(f\"Recall:    {metrics['recall']:.3f} ({metrics['recall']*100:.1f}%)\")\n",
    "    print(f\"F1-Score:  {metrics['f1_score']:.3f} ({metrics['f1_score']*100:.1f}%)\")\n",
    "    print(f\"AUC:       {metrics['auc']:.3f}\")\n",
    "    \n",
    "    cm = metrics['confusion_matrix']\n",
    "    print(f\"\\n📊 Confusion Matrix:\")\n",
    "    print(f\"  True Positives:  {cm['true_positives']}\")\n",
    "    print(f\"  False Positives: {cm['false_positives']}\")\n",
    "    print(f\"  True Negatives:  {cm['true_negatives']}\")\n",
    "    print(f\"  False Negatives: {cm['false_negatives']}\")\n",
    "    \n",
    "    print(f\"\\n📈 Detection Summary:\")\n",
    "    print(f\"  Anomalies detected: {metrics['anomalies_detected']}\")\n",
    "    print(f\"  Actual anomalies:   {metrics['actual_anomalies']}\")\n",
    "    print(f\"  Total samples:      {metrics['total_samples']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data and len(data) > 0:\n",
    "    # Create visualizations\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    fig.suptitle('Isolation Forest Anomaly Detection Results', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Anomaly Score Distribution\n",
    "    axes[0, 0].hist(scores, bins=50, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    axes[0, 0].axvline(metrics['threshold'], color='red', linestyle='--', linewidth=2, label=f'Threshold: {metrics[\"threshold\"]:.3f}')\n",
    "    axes[0, 0].set_xlabel('Anomaly Score')\n",
    "    axes[0, 0].set_ylabel('Frequency')\n",
    "    axes[0, 0].set_title('Distribution of Anomaly Scores')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Confusion Matrix Heatmap\n",
    "    cm_matrix = np.array([[cm['true_negatives'], cm['false_positives']], \n",
    "                         [cm['false_negatives'], cm['true_positives']]])\n",
    "    sns.heatmap(cm_matrix, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['Predicted Normal', 'Predicted Anomaly'],\n",
    "                yticklabels=['Actual Normal', 'Actual Anomaly'],\n",
    "                ax=axes[0, 1])\n",
    "    axes[0, 1].set_title('Confusion Matrix')\n",
    "    \n",
    "    # 3. Price vs Volume Scatter Plot\n",
    "    prices = [d['price'] for d in data]\n",
    "    volumes = [d['volume'] for d in data]\n",
    "    colors = ['red' if score > metrics['threshold'] else 'blue' for score in scores]\n",
    "    \n",
    "    scatter = axes[1, 0].scatter(prices, volumes, c=colors, alpha=0.6, s=30)\n",
    "    axes[1, 0].set_xlabel('Price ($)')\n",
    "    axes[1, 0].set_ylabel('Volume')\n",
    "    axes[1, 0].set_title('Price vs Volume (Red = Detected Anomalies)')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Performance Metrics Bar Chart\n",
    "    metrics_names = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC']\n",
    "    metrics_values = [metrics['accuracy'], metrics['precision'], metrics['recall'], \n",
    "                     metrics['f1_score'], metrics['auc']]\n",
    "    \n",
    "    bars = axes[1, 1].bar(metrics_names, metrics_values, color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd'])\n",
    "    axes[1, 1].set_ylabel('Score')\n",
    "    axes[1, 1].set_title('Model Performance Metrics')\n",
    "    axes[1, 1].set_ylim(0, 1)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, value in zip(bars, metrics_values):\n",
    "        axes[1, 1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "                        f'{value:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"📊 Visualizations generated successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Detailed Anomaly Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_anomalies_detailed(data: List[Dict[str, Any]], scores: List[float], \n",
    "                              threshold: float) -> None:\n",
    "    \"\"\"Analyze and display detailed anomaly information.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"🔍 DETAILED ANOMALY ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    detected_anomalies = []\n",
    "    missed_anomalies = []\n",
    "    false_positives = []\n",
    "    \n",
    "    for i, record in enumerate(data):\n",
    "        predicted = scores[i] > threshold\n",
    "        actual = record.get('is_anomaly', False)\n",
    "        \n",
    "        if predicted and actual:\n",
    "            detected_anomalies.append((record, scores[i]))\n",
    "        elif not predicted and actual:\n",
    "            missed_anomalies.append((record, scores[i]))\n",
    "        elif predicted and not actual:\n",
    "            false_positives.append((record, scores[i]))\n",
    "    \n",
    "    # Sort by anomaly score\n",
    "    detected_anomalies.sort(key=lambda x: x[1], reverse=True)\n",
    "    missed_anomalies.sort(key=lambda x: x[1], reverse=True)\n",
    "    false_positives.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Show correctly detected anomalies\n",
    "    print(f\"\\n✅ CORRECTLY DETECTED ANOMALIES ({len(detected_anomalies)}):\")\n",
    "    print(\"-\" * 60)\n",
    "    for i, (record, score) in enumerate(detected_anomalies[:5]):  # Show top 5\n",
    "        print(f\"\\n{i+1}. {record['symbol']} (Score: {score:.4f})\")\n",
    "        print(f\"   💰 Price: ${record['price']:.2f}, 📊 Volume: {record['volume']:,}\")\n",
    "        print(f\"   🏷️  Type: {record.get('anomaly_type', 'unknown')}\")\n",
    "        for reason in record.get('anomaly_reasons', [])[:2]:  # Show first 2 reasons\n",
    "            print(f\"   • {reason}\")\n",
    "    \n",
    "    # Show missed anomalies\n",
    "    if missed_anomalies:\n",
    "        print(f\"\\n❌ MISSED ANOMALIES ({len(missed_anomalies)}):\")\n",
    "        print(\"-\" * 40)\n",
    "        for i, (record, score) in enumerate(missed_anomalies[:3]):  # Show top 3\n",
    "            print(f\"\\n{i+1}. {record['symbol']} (Score: {score:.4f} - below threshold)\")\n",
    "            print(f\"   🏷️  Type: {record.get('anomaly_type', 'unknown')}\")\n",
    "            for reason in record.get('anomaly_reasons', [])[:2]:\n",
    "                print(f\"   • {reason}\")\n",
    "    \n",
    "    # Show false positives\n",
    "    if false_positives:\n",
    "        print(f\"\\n⚠️  FALSE POSITIVES ({len(false_positives)}):\")\n",
    "        print(\"-\" * 35)\n",
    "        for i, (record, score) in enumerate(false_positives[:3]):  # Show top 3\n",
    "            print(f\"\\n{i+1}. {record['symbol']} (Score: {score:.4f})\")\n",
    "            print(f\"   💰 Price: ${record['price']:.2f}, 📊 Volume: {record['volume']:,}\")\n",
    "            print(f\"   📈 RSI: {record['rsi']:.1f}, 📉 Volatility: {record['volatility']*100:.1f}%\")\n",
    "\n",
    "if data:\n",
    "    analyze_anomalies_detailed(data, scores, metrics['threshold'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data:\n",
    "    # Analyze feature distributions for anomalies vs normal trades\n",
    "    print(\"\\n🔬 FEATURE ANALYSIS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Separate anomalies and normal trades\n",
    "    anomalies_df = df[df['is_anomaly'] == True]\n",
    "    normal_df = df[df['is_anomaly'] == False]\n",
    "    \n",
    "    # Key features to analyze\n",
    "    key_features = ['price', 'volume', 'volatility', 'rsi', 'macd', 'liquidity_ratio']\n",
    "    \n",
    "    print(\"\\n📊 Feature Statistics Comparison:\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for feature in key_features:\n",
    "        if feature in df.columns:\n",
    "            normal_mean = normal_df[feature].mean()\n",
    "            anomaly_mean = anomalies_df[feature].mean()\n",
    "            difference = ((anomaly_mean - normal_mean) / normal_mean) * 100\n",
    "            \n",
    "            print(f\"{feature.upper():>15}: Normal={normal_mean:>8.2f}, Anomaly={anomaly_mean:>8.2f}, Diff={difference:>+6.1f}%\")\n",
    "    \n",
    "    # Create feature comparison plot\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "    fig.suptitle('Feature Distributions: Normal vs Anomalous Trades', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    for i, feature in enumerate(key_features):\n",
    "        if feature in df.columns:\n",
    "            row, col = i // 3, i % 3\n",
    "            \n",
    "            # Plot histograms\n",
    "            axes[row, col].hist(normal_df[feature], bins=30, alpha=0.7, label='Normal', color='blue', density=True)\n",
    "            axes[row, col].hist(anomalies_df[feature], bins=30, alpha=0.7, label='Anomaly', color='red', density=True)\n",
    "            \n",
    "            axes[row, col].set_xlabel(feature.replace('_', ' ').title())\n",
    "            axes[row, col].set_ylabel('Density')\n",
    "            axes[row, col].set_title(f'{feature.replace(\"_\", \" \").title()} Distribution')\n",
    "            axes[row, col].legend()\n",
    "            axes[row, col].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n📈 Feature analysis completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data:\n",
    "    # Save results\n",
    "    results = {\n",
    "        'model_parameters': {\n",
    "            'n_trees': model.n_trees,\n",
    "            'subsample_size': model.subsample_size,\n",
    "            'max_depth': model.max_depth,\n",
    "            'contamination': model.contamination\n",
    "        },\n",
    "        'metrics': metrics,\n",
    "        'predictions': [\n",
    "            {\n",
    "                'id': data[i]['id'],\n",
    "                'symbol': data[i]['symbol'],\n",
    "                'anomaly_score': scores[i],\n",
    "                'predicted_anomaly': scores[i] > metrics['threshold'],\n",
    "                'actual_anomaly': data[i].get('is_anomaly', False)\n",
    "            }\n",
    "            for i in range(len(data))\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    with open('isolation_forest_results.json', 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    \n",
    "    print(\"💾 Results saved to 'isolation_forest_results.json'\")\n",
    "    print(\"✅ Analysis complete!\")\n",
    "    \n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"📋 ANALYSIS SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"🎯 Model Accuracy: {metrics['accuracy']*100:.1f}%\")\n",
    "    print(f\"🎯 Precision: {metrics['precision']*100:.1f}%\")\n",
    "    print(f\"🎯 Recall: {metrics['recall']*100:.1f}%\")\n",
    "    print(f\"🎯 F1-Score: {metrics['f1_score']*100:.1f}%\")\n",
    "    print(f\"📊 Total Trades Analyzed: {len(data)}\")\n",
    "    print(f\"🚨 Anomalies Detected: {metrics['anomalies_detected']}\")\n",
    "    print(f\"✅ True Anomalies: {metrics['actual_anomalies']}\")\n",
    "else:\n",
    "    print(\"❌ No data available for analysis. Please ensure the dataset file exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Model Interpretation and Insights\n",
    "\n",
    "### How Isolation Forest Works:\n",
    "1. **Random Sampling**: Each tree uses a random subset of data\n",
    "2. **Random Feature Selection**: At each split, randomly choose a feature\n",
    "3. **Random Threshold**: Set random split points between min/max values\n",
    "4. **Path Length**: Anomalies have shorter paths (easier to isolate)\n",
    "5. **Ensemble Scoring**: Average path lengths across all trees\n",
    "\n",
    "### Key Advantages:\n",
    "- ✅ **No labeled data required** (unsupervised learning)\n",
    "- ✅ **Handles high-dimensional data** efficiently\n",
    "- ✅ **Linear time complexity** O(n log n)\n",
    "- ✅ **Robust to outliers** in training data\n",
    "- ✅ **Interpretable results** with clear anomaly scores\n",
    "\n",
    "### Business Applications:\n",
    "- 🏦 **Fraud Detection**: Identify suspicious transactions\n",
    "- 📈 **Market Surveillance**: Detect market manipulation\n",
    "- ⚠️ **Risk Management**: Flag high-risk trading patterns\n",
    "- 🔍 **Compliance**: Monitor for regulatory violations\n",
    "- 🤖 **Algorithmic Trading**: Identify market anomalies for trading signals"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
